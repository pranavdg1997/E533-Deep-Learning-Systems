{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6LHUwv1WVpvC",
    "outputId": "858c61b7-d423-4be2-e74a-43b1e80308f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU MIL GAYA, AB MACHAYENGE!!!\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import *\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Importing pytorch functions and modules\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.utils.data \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import librosa\n",
    "from librosa.core import stft,istft\n",
    "\n",
    "#Setting random seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7hrmm9FZJZH"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loader(tr_v_te,batch_size,max_len):\n",
    "  \"\"\"\n",
    "  Creates Pytorch dataloader object from the specified folder and batch size\n",
    "  Additionally, it saves the data along with respective lengths(to be used in RNN)\n",
    "  Arguments - \n",
    "  tr_v_te: which is among{tr,v,te} indicating which folder to load files from\n",
    "  batch_size: batch_size for creating data loader\n",
    "  max_len: maximum allowable length of sound signal after stft (will give error of any sound lengths exceeds this) \n",
    "\n",
    "  \"\"\"\n",
    "  n_signals = int(len(os.listdir(tr_v_te+\"/\"))/3)\n",
    "  input_dm = 513\n",
    "  #maximum length of sound file\n",
    "  loader_file = tr_v_te + \"_loader_\" +str(batch_size) + \".pt\"\n",
    "  #if data loader file already exists, read it directly instead of taking in new file\n",
    "  if(loader_file not in os.listdir()):\n",
    "      data_files = [\"seq_lengths_\"+tr_v_te+\".pt\",\"X_\"+tr_v_te+\".npy\",\"M_\"+tr_v_te+\".npy\"]\n",
    "      if(set(data_files).issubset(set(os.listdir()))):\n",
    "          seq_lengths = torch.load(data_files[0])\n",
    "          X = np.load(data_files[1])\n",
    "          M = np.load(data_files[2])\n",
    "          print(\"loaded data files\")\n",
    "      else:\n",
    "          print(\"Data files not found, reading sound files...\")\n",
    "          \n",
    "          nos = np.arange(0,n_signals).astype(np.str)\n",
    "          S_complex = np.zeros((n_signals,input_dm,max_len)).astype(np.complex64)\n",
    "          N_complex = np.zeros((n_signals,input_dm,max_len)).astype(np.complex64)\n",
    "          seq_lengths = np.zeros((n_signals)).astype(np.int)\n",
    "          for i,no in enumerate(nos):\n",
    "              s_filename = \"/\".join([tr_v_te,tr_v_te]) +\"s\" + no.zfill(4) + \".wav\"\n",
    "              s, sr=librosa.load(s_filename, sr=None)\n",
    "              s_complex = stft(s, n_fft=1024, hop_length=512)\n",
    "              seq_lengths[i] = int(s_complex.shape[1])\n",
    "              S_complex[i,:,0:seq_lengths[i]] = s_complex\n",
    "\n",
    "\n",
    "\n",
    "              n_filename = \"/\".join([tr_v_te,tr_v_te]) +\"n\" + no.zfill(4) + \".wav\"\n",
    "              n,_=librosa.load(n_filename, sr=sr)\n",
    "              n_complex = stft(n, n_fft=1024, hop_length=512)\n",
    "              assert(s_complex.shape[1]==n_complex.shape[1])\n",
    "              N_complex[i,:,0:seq_lengths[i]] = n_complex\n",
    "\n",
    "          S = np.abs(S_complex)\n",
    "          del(S_complex)\n",
    "          N = np.abs(N_complex)\n",
    "          del(N_complex)\n",
    "          M = np.greater(S,N).astype(np.float64)\n",
    "          del(S)\n",
    "          del(N)\n",
    "\n",
    "          X_complex = np.zeros((n_signals,input_dm,max_len)).astype(np.complex64)  \n",
    "          for i,no in enumerate(nos):     \n",
    "              x_filename = \"/\".join([tr_v_te,tr_v_te]) +\"x\" + no.zfill(4) + \".wav\"\n",
    "              x,_=librosa.load(x_filename, sr=sr)\n",
    "              x_complex = stft(x, n_fft=1024, hop_length=512)\n",
    "              X_complex[i,:,0:seq_lengths[i]] = x_complex\n",
    "          X = np.abs(X_complex)\n",
    "          \n",
    "\n",
    "\n",
    "          seq_lengths = torch.tensor(seq_lengths).to('cpu')\n",
    "          M = M.reshape((n_signals,max_len,input_dm))\n",
    "          X = X.reshape((n_signals,max_len,input_dm))\n",
    "          torch.save(seq_lengths,data_files[0])\n",
    "          np.save(data_files[1],X)\n",
    "          np.save(data_files[2],M)\n",
    "          print(\"Sound files read\")\n",
    "          \n",
    "          \n",
    "      assert(len(np.unique(X))>2)\n",
    "      data_ind = np.arange(0,n_signals)\n",
    "      data_loader_ind = torch.utils.data.DataLoader(dataset=data_ind, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True)\n",
    "\n",
    "      data_loader = []\n",
    "      for batch in data_loader_ind:\n",
    "          x_len = seq_lengths[batch]\n",
    "          sort_ind = torch.flip(torch.argsort(x_len),dims=[0])\n",
    "          sort_ind = sort_ind.cpu().numpy()\n",
    "          x_len = x_len[sort_ind]\n",
    "          \n",
    "          data_loader.append((torch.tensor(X[batch,:,:][sort_ind]).to(device),x_len,torch.tensor(M[batch,:,:][sort_ind]).to(device)))\n",
    "\n",
    "      data_loader = torch.utils.data.dataloader.DataLoader(data_loader)\n",
    "      del(X)\n",
    "      del(M)\n",
    "      torch.save(data_loader,loader_file)\n",
    "      print(\"created data loader:\"+loader_file)\n",
    "  \n",
    "  else:\n",
    "      print(\"data loader file found\")\n",
    "      data_loader = torch.load(loader_file)\n",
    "      \n",
    "  return(data_loader)\n",
    "\n",
    "#Calculate snr for sound signal\n",
    "def get_snr(s_actual,s_pred):\n",
    "    num = np.dot(s_actual,s_actual)\n",
    "    den = np.dot((s_actual-s_pred),(s_actual-s_pred))\n",
    "    return(10*np.log10(num/den))\n",
    "\n",
    "def get_snrs(model,tr_v_te,n_signals):\n",
    "  \"\"\"\n",
    "  Get array of SNRs of sound files from a specified folder and trained model\n",
    "  Arguments - \n",
    "  tr_v_te: which is among{tr,v,te} indicating which folder to load files from\n",
    "  model: trained pytorch model\n",
    "  n_signals: Number of signals to find SNR for(ideally this would be all the files in the folder but that often exceeds RAM and takes huge amount of time)\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  print(\"Calulcating SNR for first \"+str(n_signals)+\" files in \"+tr_v_te+\"/ folder...\")\n",
    "  nos = np.arange(0,n_signals).astype(np.str)\n",
    "  signals = []\n",
    "  S_complex = np.zeros((n_signals,513,max_len)).astype(np.complex64)\n",
    "  X_complex = np.zeros((n_signals,513,max_len)).astype(np.complex64)\n",
    "  seq_lengths = np.zeros((n_signals))\n",
    "  for i,no in enumerate(nos):\n",
    "      s_filename = \"/\".join([tr_v_te,tr_v_te]) +\"s\" + no.zfill(4) + \".wav\"\n",
    "      x_filename = \"/\".join([tr_v_te,tr_v_te]) +\"x\" + no.zfill(4) + \".wav\"\n",
    "      s, sr=librosa.load(s_filename, sr=None)\n",
    "      signals.append(s)\n",
    "      x, sr=librosa.load(x_filename, sr=None)\n",
    "      s_complex = stft(s, n_fft=1024, hop_length=512)\n",
    "      x_complex = stft(x, n_fft=1024, hop_length=512)\n",
    "      l = int(s_complex.shape[1])\n",
    "      seq_lengths[i]=l\n",
    "      S_complex[i,:,0:l] = s_complex\n",
    "      X_complex[i,:,0:l] = x_complex\n",
    "      \n",
    "  S_mag = np.abs(S_complex)\n",
    "  S_cap = np.nan_to_num(np.divide(S_complex,S_mag))\n",
    "  X_mag = np.abs(X_complex)\n",
    "  #X_cap = np.nan_to_num(np.divide(X_complex,X_mag))\n",
    "\n",
    "  X_inp = torch.tensor(X_mag).reshape(n_signals,max_len,513).to(device)\n",
    "  X_len = torch.tensor(seq_lengths,dtype=torch.int64)\n",
    "  sort_ind = torch.flip(torch.argsort(X_len),dims=[0])\n",
    "  X_len = X_len[sort_ind]\n",
    "  X_inp = X_inp[sort_ind]\n",
    "\n",
    "  M_hat = F.sigmoid(model.forward(X_inp,X_len)).gt(0.5).int()\n",
    "\n",
    "\n",
    "\n",
    "  S_complex = S_complex[sort_ind]\n",
    "  S_mag = S_mag[sort_ind]\n",
    "  S_cap = S_cap[sort_ind]\n",
    "  signals = [signals[i] for i in sort_ind]\n",
    "  seq_lengths = X_len.numpy()\n",
    "\n",
    "  S_mag_pred = torch.mul(X_inp,M_hat).reshape(n_signals,513,max_len).detach().cpu().numpy()\n",
    "  S_complex_pred = np.multiply(S_mag_pred,S_cap)\n",
    "\n",
    "  SNR_list = np.zeros((n_signals))\n",
    "  for i in range(0,n_signals):\n",
    "      s_complex_pred = S_complex_pred[i,:,0:seq_lengths[i]]\n",
    "      \n",
    "      s = signals[i]\n",
    "      s_pred = istft(s_complex_pred, hop_length=512, length = len(s))\n",
    "      \n",
    "      SNR_list[i] = get_snr(s,s_pred)\n",
    "  \n",
    "  print(\"SNR mean = \" + str(np.mean(SNR_list)))\n",
    "  print(\"SNR median = \" + str(np.median(SNR_list)))\n",
    "\n",
    "  return(SNR_list)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class denoise_RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch model class for removing noise from sound\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dm, n_lstm_layers, n_lstm_units, output_dm,dropout,max_len):\n",
    "        \n",
    "        super(denoise_RNN,self).__init__()\n",
    "        self.n_lstm_units = n_lstm_units\n",
    "        self.output_dm = output_dm\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dm,\n",
    "            hidden_size=n_lstm_units,\n",
    "            num_layers=n_lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout = self.dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # output layer which projects back to tag space\n",
    "        self.fc = nn.Linear(self.n_lstm_units*self.max_len*2,self.output_dm*self.max_len)\n",
    "        self.dd = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        \n",
    "        X_packed = nn.utils.rnn.pack_padded_sequence(X,X_lengths, batch_first=True)\n",
    "        X_packed, (hidden, cell) = self.lstm(X_packed)\n",
    "        X,_ = nn.utils.rnn.pad_packed_sequence(X_packed, batch_first=True, \n",
    "                                                   padding_value=0.0, total_length=self.max_len)\n",
    "        X = X.reshape(X.shape[0],-1)\n",
    "        X = self.fc(X)\n",
    "        #Instead of taking sigmoid here we use loss function BCE with logits because although both processes\n",
    "        # are the same, the latter is more stable and does go into errors caused my rounding off of digits.\n",
    "\n",
    "        X = self.dd(X)\n",
    "        M_hat = X.reshape(X.shape[0],self.max_len,self.output_dm)\n",
    "        return M_hat       \n",
    "\n",
    "\n",
    "class train_lstm:\n",
    "    \"\"\"\n",
    "    Class for training an LSTM model(takes in the above specified lstm model class)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,batch_size,dropout,n_lstm_layers,n_lstm_units,input_dm,output_dm,max_len,saved_model):\n",
    "        super(train_lstm,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dm = output_dm\n",
    "        \n",
    "        self.n_lstm_units = n_lstm_units\n",
    "        #Load or create pytorch data loader files based n the specified batch_size\n",
    "        self.train_loader = get_data_loader(\"tr\",batch_size,max_len)\n",
    "        self.val_loader = get_data_loader(\"v\",batch_size,max_len)\n",
    "        \n",
    "        self.n_train = np.sum([len(x.squeeze()) for (x,xl,m) in self.train_loader])\n",
    "        self.n_val = np.sum([len(x.squeeze()) for (x,xl,m) in self.val_loader])\n",
    "        \n",
    "        self.rnn_model = denoise_RNN(input_dm=input_dm, \n",
    "                        n_lstm_layers=n_lstm_layers,\n",
    "                        n_lstm_units=self.n_lstm_units,\n",
    "                        output_dm=output_dm,\n",
    "                        dropout=dropout,\n",
    "                        max_len=max_len).to(device)\n",
    "    \n",
    "        self.best_loss = torch.tensor(np.inf)\n",
    "        if(saved_model!=None):\n",
    "            self.rnn_model.load_state_dict(torch.load(saved_model))\n",
    "        \n",
    "        \n",
    "    def train(self,max_epochs,verbose,learning_rate,model_op):\n",
    "        \n",
    "\n",
    "        optimizer = torch.optim.Adam(self.rnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "        criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "\n",
    "        \n",
    "        epoch = 0\n",
    "        rounds = 0\n",
    "        early_stopping_rounds = 5\n",
    "        stop = False\n",
    "        \n",
    "\n",
    "        while ((epoch < max_epochs)&(stop==False)):\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for x_input,x_length,m in self.train_loader:\n",
    "                x_input = x_input.squeeze()\n",
    "                x_length = x_length.squeeze()\n",
    "                m = m.squeeze()\n",
    "     \n",
    "                m_pred = self.rnn_model.forward(x_input,x_length)\n",
    "\n",
    "                batch_ce_loss = 0.0\n",
    "\n",
    "                for i in range(m_pred.size(0)):\n",
    "                    m_pred_up = m_pred[i,0:x_length[i],:].reshape(1,x_length[i]*self.output_dm)\n",
    "                    m_up = m[i,0:x_length[i],:].reshape(1,x_length[i]*self.output_dm).float()\n",
    "                    #print(\"reshape successfull\")\n",
    "                    ce_loss = F.binary_cross_entropy_with_logits(m_pred_up,m_up,reduction=\"sum\")\n",
    "                    # ce_loss = criterion(m_pred[i,0:x_length[i],:].view(1,-1), \n",
    "                    #                     m[i,0:x_length[i],:].view(1,-1).float())\n",
    "                    batch_ce_loss += ce_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_ce_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss = total_loss + batch_ce_loss.item()\n",
    "            train_loss = total_loss/self.n_train\n",
    "            self.train_loss_history.append(train_loss.item())\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for v_input,v_length,m in self.val_loader:\n",
    "                v_input = v_input.squeeze()\n",
    "                v_length = v_length.squeeze()\n",
    "                m = m.squeeze()\n",
    "                m_pred = self.rnn_model.forward(v_input,v_length)\n",
    "                batch_ce_loss = 0.0\n",
    "                \n",
    "                for i in range(m_pred.size(0)):\n",
    "                    \n",
    "                    m_pred_up = m_pred[i,0:v_length[i],:].reshape(1,v_length[i]*self.output_dm)\n",
    "                    m_up = m[i,0:v_length[i],:].reshape(1,v_length[i]*self.output_dm).float()\n",
    "                    #print(\"reshape successfull\")\n",
    "                    ce_loss = F.binary_cross_entropy_with_logits(m_pred_up,m_up,reduction=\"sum\")\n",
    "\n",
    "                    batch_ce_loss += ce_loss\n",
    "\n",
    "                total_loss = total_loss + batch_ce_loss.item()\n",
    "            val_loss = total_loss/self.n_val\n",
    "            self.val_loss_history.append(val_loss.item())\n",
    "\n",
    "\n",
    "            if(val_loss<self.best_loss):\n",
    "                self.best_loss = val_loss\n",
    "                rounds = 0\n",
    "            else:\n",
    "                rounds += 1\n",
    "\n",
    "\n",
    "            if(rounds>=early_stopping_rounds):\n",
    "                stop=True\n",
    "                \n",
    "            \n",
    "            if((verbose==True)&((epoch%5==0)|(stop==True))):\n",
    "                print(\"----------------------------------------------------------------------------------\")\n",
    "                print(\"EPOCH:\"+str(epoch))\n",
    "                if(stop==True):\n",
    "                    print(\"Training to be concluded after this epoch\") \n",
    "                print(\"Average training loss per sample  = \"+str(train_loss))\n",
    "                print(\"Average Validation loss per sample  = \"+str(val_loss))\n",
    "\n",
    "                #get SNR for 100 samples from validation folder\n",
    "                snr_list = get_snrs(self.rnn_model,\"v\",20)\n",
    "\n",
    "\n",
    "            epoch += 1\n",
    "        self.trained_model = self.rnn_model\n",
    "        \n",
    "        if(verbose==True):\n",
    "            print(\"Training complete\")\n",
    "        if(model_op!= None):\n",
    "            torch.save(self.rnn_model.state_dict(),model_op)\n",
    "            if(verbose==True):\n",
    "                print(\"saved_model\")\n",
    "            \n",
    "            \n",
    "        \n",
    "    def plot_loss(self):\n",
    "        print(\"----------------------------------------------------------------------------------\")\n",
    "        plt.plot(self.train_loss_history)\n",
    "        plt.plot(self.val_loss_history)\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Xkw5YSq1cudW",
    "outputId": "ceb6083a-bdf9-4ece-fd84-5902d660ac05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loader file found\n",
      "data loader file found\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:0\n",
      "Average training loss per sample  = 28895.878333333334\n",
      "Average Validation loss per sample  = 22810.1875\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 3.091272071003914\n",
      "SNR median = 3.014088124036789\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:5\n",
      "Average training loss per sample  = 15467.120833333332\n",
      "Average Validation loss per sample  = 16371.269791666668\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 6.397171966731548\n",
      "SNR median = 6.784234642982483\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:10\n",
      "Average training loss per sample  = 11884.676458333333\n",
      "Average Validation loss per sample  = 14262.588541666666\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 8.25298086553812\n",
      "SNR median = 8.87986958026886\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:15\n",
      "Average training loss per sample  = 9696.086145833333\n",
      "Average Validation loss per sample  = 13367.233958333334\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 8.977496966719627\n",
      "SNR median = 9.991769194602966\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:20\n",
      "Average training loss per sample  = 8250.203958333334\n",
      "Average Validation loss per sample  = 12997.745208333334\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 9.399362117052078\n",
      "SNR median = 10.426382422447205\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:25\n",
      "Average training loss per sample  = 7216.5090625\n",
      "Average Validation loss per sample  = 12833.260625\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 9.680201679468155\n",
      "SNR median = 10.954152345657349\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:30\n",
      "Average training loss per sample  = 6445.883333333333\n",
      "Average Validation loss per sample  = 12780.243333333334\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 9.842404156923294\n",
      "SNR median = 10.855149030685425\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:34\n",
      "Training to be concluded after this epoch\n",
      "Average training loss per sample  = 5968.312291666667\n",
      "Average Validation loss per sample  = 13187.075416666667\n",
      "Calulcating SNR for first 20 files in v/ folder...\n",
      "SNR mean = 9.705668099224567\n",
      "SNR median = 10.78364610671997\n",
      "Training complete\n",
      "----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDrMQMcYY02EWIsYYYzrMQsQYY0yH/X/ECw4UaP87/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This harcoded combination of hyper params should give the best SNR(around 10-11)\n",
    "max_len = 195\n",
    "batch_size = 200\n",
    "n_lstm_units = 10\n",
    "learning_rate = 0.001\n",
    "dropout = 0\n",
    "\n",
    "tl = train_lstm(batch_size=batch_size,\n",
    "                dropout=dropout,\n",
    "                n_lstm_layers=1,\n",
    "                n_lstm_units=n_lstm_units,\n",
    "                input_dm=513,\n",
    "                output_dm=513,\n",
    "                max_len=max_len,\n",
    "                saved_model=None)\n",
    "tl.train(max_epochs=200,\n",
    "         verbose=True,\n",
    "         learning_rate=learning_rate,\n",
    "         model_op=None)\n",
    "tl.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_s7SBZ7MxRpW",
    "outputId": "4f5a4e70-3d81-45f1-e41f-ba7369ec335c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 400 cleaned sound files in the format:te/tes_predicted0099.wav\n",
      "Saved 400 cleaned sound files in the format:te/tes_predicted0199.wav\n",
      "Saved 400 cleaned sound files in the format:te/tes_predicted0299.wav\n",
      "Saved 400 cleaned sound files in the format:te/tes_predicted0399.wav\n"
     ]
    }
   ],
   "source": [
    "def output_cleaned_sound(model,tr_v_te,n_signals,max_len):\n",
    "  \"\"\"\n",
    "  Outputs cleaned signal for each noisy signal present in the folder specified and based on the trained model provided in input\n",
    "  The output is generate in the same folder as input\n",
    "  Arguments - \n",
    "  tr_v_te: which is among{tr,v,te} indicating which folder to load files from\n",
    "  model: trained pytorch model\n",
    "  n_signals: Number of signals to generate output for (ideally this would be all the files in the folder ie 400 for test folder but that often exceeds RAM)\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  nos = np.arange(0,n_signals).astype(np.str)\n",
    "  X_complex = np.zeros((n_signals,513,max_len)).astype(np.complex64)\n",
    "  seq_lengths = np.zeros((n_signals))\n",
    "  signals = []\n",
    "  for i in range(0,n_signals):\n",
    "      no = str(i)\n",
    "      x_filename = \"/\".join([tr_v_te,tr_v_te]) +\"x\" + no.zfill(4) + \".wav\"\n",
    "      x, sr=librosa.load(x_filename, sr=None)\n",
    "      signals.append(x)\n",
    "      x_complex = stft(x, n_fft=1024, hop_length=512)\n",
    "      l = int(x_complex.shape[1])\n",
    "      seq_lengths[i]=max(l,max_len)\n",
    "      X_complex[i,:,0:l] = x_complex\n",
    "\n",
    "\n",
    "  X_mag = np.abs(X_complex)\n",
    "  X_cap = np.nan_to_num(np.divide(X_complex,X_mag))\n",
    "\n",
    "  X_inp = torch.tensor(X_mag).reshape(n_signals,max_len,513).to(device)\n",
    "  X_len = torch.tensor(seq_lengths,dtype=torch.int64)\n",
    "  sort_ind = torch.flip(torch.argsort(X_len),dims=[0])\n",
    "  X_len = X_len[sort_ind]\n",
    "  X_inp = X_inp[sort_ind]\n",
    "  signals = [signals[j] for j in sort_ind]\n",
    "\n",
    "  \n",
    "  M_hat = F.sigmoid(model.forward(X_inp,X_len)).gt(0.5).int()\n",
    "\n",
    "  seq_lengths = X_len.numpy()\n",
    "  S_mag_pred = torch.mul(X_inp,M_hat).reshape(n_signals,513,max_len).detach().cpu().numpy()\n",
    "  S_complex_pred = np.multiply(S_mag_pred,X_cap)\n",
    "  for i in range(0,n_signals):\n",
    "      no = str(i)\n",
    "      op_filename = \"/\".join([tr_v_te,tr_v_te]) + \"s_predicted\" + no.zfill(4) + \".wav\"\n",
    "      s_complex_pred = S_complex_pred[i,:,0:seq_lengths[i]]\n",
    "      s_pred = istft(s_complex_pred, hop_length=512, length = len(signals[i]))\n",
    "      librosa.output.write_wav(op_filename,s_pred, sr)\n",
    "      if((i+1)%100==0):\n",
    "        print(\"Saved \"+str(i)+ \" cleaned sound files in the format:\"+op_filename)\n",
    "\n",
    "\n",
    "output_cleaned_sound(tl.trained_model,\"te\",400,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5yplRD2FPWv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Assignment-3-problem2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
